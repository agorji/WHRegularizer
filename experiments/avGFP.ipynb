{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93e5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import ModelTrainer\n",
    "from datasets import avGFPDataset, GB1Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9c40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, n, multiplier=2, batch_norm=False):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n, multiplier*n)\n",
    "        self.fc2 = nn.Linear(multiplier*n, multiplier*n)\n",
    "        self.fc3 = nn.Linear(multiplier*n, n)\n",
    "        self.fc4 = nn.Linear(n, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(multiplier*n)\n",
    "            self.bn2 = nn.BatchNorm1d(multiplier*n)\n",
    "            self.bn3 = nn.BatchNorm1d(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(F.leaky_relu(self.fc1(x)))\n",
    "            x = self.bn2(F.leaky_relu(self.fc2(x)))\n",
    "            x = self.bn3(F.leaky_relu(self.fc3(x)))\n",
    "            x = self.fc4(x)\n",
    "        else:\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            x = F.leaky_relu(self.fc2(x))\n",
    "            x = F.leaky_relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "\n",
    "        return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc3dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from cache.\n",
      "Loading SPRIGHT samples from cache ...\n",
      "#0 - Train Loss: 0.199, R2: -0.054\tValidation Loss: 0.134, R2: 0.137\n",
      "#1 - Train Loss: 0.133, R2: 0.295\tValidation Loss: 0.101, R2: 0.351\n",
      "#2 - Train Loss: 0.059, R2: 0.687\tValidation Loss: 0.096, R2: 0.384\n",
      "#3 - Train Loss: 0.025, R2: 0.867\tValidation Loss: 0.094, R2: 0.397\n",
      "#4 - Train Loss: 0.025, R2: 0.868\tValidation Loss: 0.092, R2: 0.411\n",
      "#5 - Train Loss: 0.026, R2: 0.864\tValidation Loss: 0.089, R2: 0.427\n",
      "#6 - Train Loss: 0.010, R2: 0.949\tValidation Loss: 0.080, R2: 0.487\n",
      "#7 - Train Loss: 0.007, R2: 0.964\tValidation Loss: 0.085, R2: 0.458\n",
      "#8 - Train Loss: 0.006, R2: 0.970\tValidation Loss: 0.077, R2: 0.509\n",
      "#9 - Train Loss: 0.005, R2: 0.976\tValidation Loss: 0.083, R2: 0.467\n",
      "#10 - Train Loss: 0.006, R2: 0.970\tValidation Loss: 0.084, R2: 0.460\n",
      "#11 - Train Loss: 0.005, R2: 0.975\tValidation Loss: 0.080, R2: 0.486\n",
      "#12 - Train Loss: 0.003, R2: 0.984\tValidation Loss: 0.083, R2: 0.466\n",
      "#13 - Train Loss: 0.003, R2: 0.984\tValidation Loss: 0.080, R2: 0.489\n",
      "#14 - Train Loss: 0.003, R2: 0.985\tValidation Loss: 0.081, R2: 0.478\n",
      "#15 - Train Loss: 0.003, R2: 0.985\tValidation Loss: 0.082, R2: 0.472\n",
      "#16 - Train Loss: 0.003, R2: 0.983\tValidation Loss: 0.077, R2: 0.507\n",
      "#17 - Train Loss: 0.003, R2: 0.983\tValidation Loss: 0.082, R2: 0.472\n",
      "#18 - Train Loss: 0.005, R2: 0.976\tValidation Loss: 0.083, R2: 0.470\n",
      "#19 - Train Loss: 0.005, R2: 0.971\tValidation Loss: 0.081, R2: 0.481\n",
      "#20 - Train Loss: 0.004, R2: 0.981\tValidation Loss: 0.081, R2: 0.480\n",
      "#21 - Train Loss: 0.005, R2: 0.975\tValidation Loss: 0.078, R2: 0.503\n",
      "#22 - Train Loss: 0.004, R2: 0.978\tValidation Loss: 0.082, R2: 0.474\n",
      "#23 - Train Loss: 0.005, R2: 0.973\tValidation Loss: 0.084, R2: 0.464\n",
      "#24 - Train Loss: 0.005, R2: 0.975\tValidation Loss: 0.079, R2: 0.496\n",
      "#25 - Train Loss: 0.006, R2: 0.971\tValidation Loss: 0.085, R2: 0.452\n",
      "#26 - Train Loss: 0.006, R2: 0.971\tValidation Loss: 0.075, R2: 0.517\n",
      "#27 - Train Loss: 0.006, R2: 0.970\tValidation Loss: 0.088, R2: 0.438\n",
      "#28 - Train Loss: 0.006, R2: 0.967\tValidation Loss: 0.089, R2: 0.431\n",
      "#29 - Train Loss: 0.006, R2: 0.967\tValidation Loss: 0.078, R2: 0.498\n",
      "#30 - Train Loss: 0.006, R2: 0.969\tValidation Loss: 0.088, R2: 0.433\n",
      "#31 - Train Loss: 0.007, R2: 0.961\tValidation Loss: 0.081, R2: 0.481\n",
      "#32 - Train Loss: 0.007, R2: 0.964\tValidation Loss: 0.081, R2: 0.483\n",
      "#33 - Train Loss: 0.005, R2: 0.973\tValidation Loss: 0.082, R2: 0.474\n",
      "#34 - Train Loss: 0.005, R2: 0.971\tValidation Loss: 0.082, R2: 0.476\n",
      "#35 - Train Loss: 0.007, R2: 0.961\tValidation Loss: 0.078, R2: 0.500\n"
     ]
    }
   ],
   "source": [
    "fix_seed = 1\n",
    "random_seed = 11\n",
    "\n",
    "train_size = 2000\n",
    "\n",
    "# config = {\n",
    "#     \"training_method\": \"hashing\",\n",
    "#     \"b\": 10,\n",
    "#     \"lr\": 0.01, \n",
    "#     \"weight_decay\": 0, \n",
    "#     \"hadamard_lambda\": 0.0001,\n",
    "#     \"num_epochs\": 100,\n",
    "#     \"random_seed\": random_seed,\n",
    "#     \"fix_seed\": fix_seed,\n",
    "#     \"train_size\": train_size,\n",
    "#     \"epoch_iterations\": 50,\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    \"training_method\": \"normal\",\n",
    "    \"b\": 10,\n",
    "    \"lr\": 0.01, \n",
    "    \"weight_decay\": 0, \n",
    "    \"num_epochs\": 100,\n",
    "    \"random_seed\": random_seed,\n",
    "    \"fix_seed\": fix_seed,\n",
    "    \"train_size\": train_size,\n",
    "    \"epoch_iterations\": 40,\n",
    "    \"dataset\": \"GB1\"\n",
    "}\n",
    "\n",
    "# config = {\n",
    "#     \"training_method\": \"EN-S\",\n",
    "#     \"SPRIGHT_d\": 3,\n",
    "#     \"rho\": 0.01,\n",
    "#     \"b\": 10,\n",
    "#     \"lr\": 0.01, \n",
    "#     \"weight_decay\": 0, \n",
    "#     \"hadamard_lambda\": 1,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"random_seed\": random_seed,\n",
    "#     \"fix_seed\": fix_seed,\n",
    "#     \"train_size\": 500,\n",
    "#     \"epoch_iterations\": 50,\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    \"training_method\": \"EN-S_data\",\n",
    "    \"b\": 10,\n",
    "    \"lr\": 0.01, \n",
    "    \"SPRIGHT_d\": 3,\n",
    "    \"weight_decay\": 0, \n",
    "    \"hadamard_lambda\": 0.1,\n",
    "    \"num_epochs\": 100,\n",
    "    \"random_seed\": random_seed,\n",
    "    \"fix_seed\": fix_seed,\n",
    "    \"train_size\": train_size,\n",
    "    \"epoch_iterations\": 20,\n",
    "    \"run\":2\n",
    "}\n",
    "\n",
    "    \n",
    "# Dataset\n",
    "torch.manual_seed(config[\"fix_seed\"])\n",
    "dataset = GB1Dataset()\n",
    "dataset_size = len(dataset)\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [config[\"train_size\"], dataset_size - config[\"train_size\"]])\n",
    "\n",
    "# Set batch size\n",
    "config[\"batch_size\"] = math.ceil(config[\"train_size\"] / config[\"epoch_iterations\"])\n",
    "\n",
    "# Train model\n",
    "torch.manual_seed(config[\"random_seed\"]) # Seed for network initialization\n",
    "in_dim = dataset.X.shape[1]\n",
    "model = FCN(in_dim, 1, batch_norm=False)\n",
    "trainer = ModelTrainer(model, train_ds, val_ds, config=config,  plot_results=True, checkpoint_cache=True)\n",
    "model = trainer.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e681d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
